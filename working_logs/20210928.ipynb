{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b34edd2e-5a07-41c7-9793-ba4ca5f254d3",
   "metadata": {},
   "source": [
    "## Date: 2021-09-27/ 2021-09-28\n",
    "### What to do: Finding all cosine similarity on MIT data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5af7055-9830-44d1-b184-e01e9df96606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "import os \n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import distance\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity,cosine_distances\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "raw",
   "id": "247fc701-0c1b-413c-b6a9-621389f2adac",
   "metadata": {},
   "source": [
    "'''\n",
    "# ## Task Steps: \n",
    "# 1. R-R interval 나누기\n",
    "# 2. R-R amplitude Normalization\n",
    "# 3. R-R time frame(data points) Normalization \n",
    "# 4. R-R referecne frame 잡기 -> 각각의 point에 대한 median값으로 각 point reference잡기\n",
    "\n",
    "\n",
    "# # ref. 기준점 max R = 1 \n",
    "# # cycle을 뽑아내서 \n",
    "\n",
    "# resampling \n",
    "# reference frame -> median \n",
    "# cosine similarity: 가장 작은 데이터 보기\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952a24bf-ddf6-4caf-9813-56a8a887d0ac",
   "metadata": {},
   "source": [
    "#### Arr Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61d3eed3-027d-4952-a1a3-25b3f39c9f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "PATH_IN = r'C:\\Users\\MI2RL-KHJ\\workspace_signal\\mit\\processed_arr'\n",
    "os.chdir(PATH_IN)\n",
    "rpeakList = [i for i in os.listdir(PATH_IN) if i.startswith('correct')]\n",
    "fxvalList = [i for i in os.listdir(PATH_IN) if i.startswith('filtered')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6a10c94-9c0f-486d-8b7c-31c1de562c21",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PeakDataPath</th>\n",
       "      <th>ValueDataPath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_00.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_00.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_01.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_01.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_02.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_02.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_03.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_03.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_04.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_04.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_05.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_05.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_06.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_06.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_07.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_07.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_08.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_08.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_09.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_09.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_10.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_10.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_11.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_11.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_12.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_12.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_13.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_13.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_14.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_14.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_15.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_15.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_16.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_16.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_17.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_17.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_18.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_18.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_19.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_19.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_20.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_20.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_21.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_21.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_22.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_22.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_23.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_23.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_24.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_24.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_25.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_25.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_26.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_26.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_27.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_27.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_28.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_28.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_29.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_29.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_30.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_30.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_31.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_31.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_32.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_32.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_33.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_33.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_34.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_34.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_35.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_35.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_36.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_36.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_37.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_37.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_38.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_38.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_39.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_39.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_40.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_40.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_41.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_41.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_42.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_42.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_43.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_43.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_44.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_44.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_45.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_45.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_46.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_46.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>correct_rpeaks_test_arr_ecg_arr_47.csv</td>\n",
       "      <td>filtered_test_arr_ecg_arr_47.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              PeakDataPath                     ValueDataPath\n",
       "0   correct_rpeaks_test_arr_ecg_arr_00.csv  filtered_test_arr_ecg_arr_00.csv\n",
       "1   correct_rpeaks_test_arr_ecg_arr_01.csv  filtered_test_arr_ecg_arr_01.csv\n",
       "2   correct_rpeaks_test_arr_ecg_arr_02.csv  filtered_test_arr_ecg_arr_02.csv\n",
       "3   correct_rpeaks_test_arr_ecg_arr_03.csv  filtered_test_arr_ecg_arr_03.csv\n",
       "4   correct_rpeaks_test_arr_ecg_arr_04.csv  filtered_test_arr_ecg_arr_04.csv\n",
       "5   correct_rpeaks_test_arr_ecg_arr_05.csv  filtered_test_arr_ecg_arr_05.csv\n",
       "6   correct_rpeaks_test_arr_ecg_arr_06.csv  filtered_test_arr_ecg_arr_06.csv\n",
       "7   correct_rpeaks_test_arr_ecg_arr_07.csv  filtered_test_arr_ecg_arr_07.csv\n",
       "8   correct_rpeaks_test_arr_ecg_arr_08.csv  filtered_test_arr_ecg_arr_08.csv\n",
       "9   correct_rpeaks_test_arr_ecg_arr_09.csv  filtered_test_arr_ecg_arr_09.csv\n",
       "10  correct_rpeaks_test_arr_ecg_arr_10.csv  filtered_test_arr_ecg_arr_10.csv\n",
       "11  correct_rpeaks_test_arr_ecg_arr_11.csv  filtered_test_arr_ecg_arr_11.csv\n",
       "12  correct_rpeaks_test_arr_ecg_arr_12.csv  filtered_test_arr_ecg_arr_12.csv\n",
       "13  correct_rpeaks_test_arr_ecg_arr_13.csv  filtered_test_arr_ecg_arr_13.csv\n",
       "14  correct_rpeaks_test_arr_ecg_arr_14.csv  filtered_test_arr_ecg_arr_14.csv\n",
       "15  correct_rpeaks_test_arr_ecg_arr_15.csv  filtered_test_arr_ecg_arr_15.csv\n",
       "16  correct_rpeaks_test_arr_ecg_arr_16.csv  filtered_test_arr_ecg_arr_16.csv\n",
       "17  correct_rpeaks_test_arr_ecg_arr_17.csv  filtered_test_arr_ecg_arr_17.csv\n",
       "18  correct_rpeaks_test_arr_ecg_arr_18.csv  filtered_test_arr_ecg_arr_18.csv\n",
       "19  correct_rpeaks_test_arr_ecg_arr_19.csv  filtered_test_arr_ecg_arr_19.csv\n",
       "20  correct_rpeaks_test_arr_ecg_arr_20.csv  filtered_test_arr_ecg_arr_20.csv\n",
       "21  correct_rpeaks_test_arr_ecg_arr_21.csv  filtered_test_arr_ecg_arr_21.csv\n",
       "22  correct_rpeaks_test_arr_ecg_arr_22.csv  filtered_test_arr_ecg_arr_22.csv\n",
       "23  correct_rpeaks_test_arr_ecg_arr_23.csv  filtered_test_arr_ecg_arr_23.csv\n",
       "24  correct_rpeaks_test_arr_ecg_arr_24.csv  filtered_test_arr_ecg_arr_24.csv\n",
       "25  correct_rpeaks_test_arr_ecg_arr_25.csv  filtered_test_arr_ecg_arr_25.csv\n",
       "26  correct_rpeaks_test_arr_ecg_arr_26.csv  filtered_test_arr_ecg_arr_26.csv\n",
       "27  correct_rpeaks_test_arr_ecg_arr_27.csv  filtered_test_arr_ecg_arr_27.csv\n",
       "28  correct_rpeaks_test_arr_ecg_arr_28.csv  filtered_test_arr_ecg_arr_28.csv\n",
       "29  correct_rpeaks_test_arr_ecg_arr_29.csv  filtered_test_arr_ecg_arr_29.csv\n",
       "30  correct_rpeaks_test_arr_ecg_arr_30.csv  filtered_test_arr_ecg_arr_30.csv\n",
       "31  correct_rpeaks_test_arr_ecg_arr_31.csv  filtered_test_arr_ecg_arr_31.csv\n",
       "32  correct_rpeaks_test_arr_ecg_arr_32.csv  filtered_test_arr_ecg_arr_32.csv\n",
       "33  correct_rpeaks_test_arr_ecg_arr_33.csv  filtered_test_arr_ecg_arr_33.csv\n",
       "34  correct_rpeaks_test_arr_ecg_arr_34.csv  filtered_test_arr_ecg_arr_34.csv\n",
       "35  correct_rpeaks_test_arr_ecg_arr_35.csv  filtered_test_arr_ecg_arr_35.csv\n",
       "36  correct_rpeaks_test_arr_ecg_arr_36.csv  filtered_test_arr_ecg_arr_36.csv\n",
       "37  correct_rpeaks_test_arr_ecg_arr_37.csv  filtered_test_arr_ecg_arr_37.csv\n",
       "38  correct_rpeaks_test_arr_ecg_arr_38.csv  filtered_test_arr_ecg_arr_38.csv\n",
       "39  correct_rpeaks_test_arr_ecg_arr_39.csv  filtered_test_arr_ecg_arr_39.csv\n",
       "40  correct_rpeaks_test_arr_ecg_arr_40.csv  filtered_test_arr_ecg_arr_40.csv\n",
       "41  correct_rpeaks_test_arr_ecg_arr_41.csv  filtered_test_arr_ecg_arr_41.csv\n",
       "42  correct_rpeaks_test_arr_ecg_arr_42.csv  filtered_test_arr_ecg_arr_42.csv\n",
       "43  correct_rpeaks_test_arr_ecg_arr_43.csv  filtered_test_arr_ecg_arr_43.csv\n",
       "44  correct_rpeaks_test_arr_ecg_arr_44.csv  filtered_test_arr_ecg_arr_44.csv\n",
       "45  correct_rpeaks_test_arr_ecg_arr_45.csv  filtered_test_arr_ecg_arr_45.csv\n",
       "46  correct_rpeaks_test_arr_ecg_arr_46.csv  filtered_test_arr_ecg_arr_46.csv\n",
       "47  correct_rpeaks_test_arr_ecg_arr_47.csv  filtered_test_arr_ecg_arr_47.csv"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.DataFrame({'PeakDataPath': rpeakList, 'ValueDataPath': fxvalList})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "b43ce0db-a45a-4589-95fe-1b9e8619bee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arr_00 is done. ; the length of Reference Frame: 287\n",
      "arr_01 is done. ; the length of Reference Frame: 352\n",
      "arr_02 is done. ; the length of Reference Frame: 297\n",
      "arr_03 is done. ; the length of Reference Frame: 313\n",
      "arr_04 is done. ; the length of Reference Frame: 292\n",
      "arr_05 is done. ; the length of Reference Frame: 253\n",
      "arr_06 is done. ; the length of Reference Frame: 341\n",
      "arr_07 is done. ; the length of Reference Frame: 305\n",
      "arr_08 is done. ; the length of Reference Frame: 369\n",
      "arr_09 is done. ; the length of Reference Frame: 257\n",
      "arr_10 is done. ; the length of Reference Frame: 306\n",
      "arr_11 is done. ; the length of Reference Frame: 256\n",
      "arr_12 is done. ; the length of Reference Frame: 365\n",
      "arr_13 is done. ; the length of Reference Frame: 342\n",
      "arr_14 is done. ; the length of Reference Frame: 328\n",
      "arr_15 is done. ; the length of Reference Frame: 269\n",
      "arr_16 is done. ; the length of Reference Frame: 425\n",
      "arr_17 is done. ; the length of Reference Frame: 290\n",
      "arr_18 is done. ; the length of Reference Frame: 325\n",
      "arr_19 is done. ; the length of Reference Frame: 359\n",
      "arr_20 is done. ; the length of Reference Frame: 262\n",
      "arr_21 is done. ; the length of Reference Frame: 432\n",
      "arr_22 is done. ; the length of Reference Frame: 404\n",
      "arr_23 is done. ; the length of Reference Frame: 242\n",
      "arr_24 is done. ; the length of Reference Frame: 321\n",
      "arr_25 is done. ; the length of Reference Frame: 323\n",
      "arr_26 is done. ; the length of Reference Frame: 225\n",
      "arr_27 is done. ; the length of Reference Frame: 243\n",
      "arr_28 is done. ; the length of Reference Frame: 328\n",
      "arr_29 is done. ; the length of Reference Frame: 224\n",
      "arr_30 is done. ; the length of Reference Frame: 224\n",
      "arr_31 is done. ; the length of Reference Frame: 248\n",
      "arr_32 is done. ; the length of Reference Frame: 236\n",
      "arr_33 is done. ; the length of Reference Frame: 200\n",
      "arr_34 is done. ; the length of Reference Frame: 285\n",
      "arr_35 is done. ; the length of Reference Frame: 192\n",
      "arr_36 is done. ; the length of Reference Frame: 300\n",
      "arr_37 is done. ; the length of Reference Frame: 290\n",
      "arr_38 is done. ; the length of Reference Frame: 321\n",
      "arr_39 is done. ; the length of Reference Frame: 265\n",
      "arr_40 is done. ; the length of Reference Frame: 274\n",
      "arr_41 is done. ; the length of Reference Frame: 251\n",
      "arr_42 is done. ; the length of Reference Frame: 315\n",
      "arr_43 is done. ; the length of Reference Frame: 286\n",
      "arr_44 is done. ; the length of Reference Frame: 352\n",
      "arr_45 is done. ; the length of Reference Frame: 266\n",
      "arr_46 is done. ; the length of Reference Frame: 212\n",
      "arr_47 is done. ; the length of Reference Frame: 237\n"
     ]
    }
   ],
   "source": [
    "from scipy import signal\n",
    "\n",
    "# results = 'Subejct' + ',' +  'Resampling Rate' + ',' +  'Original Points'  + ',' + 'Cosine_Similarity'  + ',' + '\\n'\n",
    "# with open (os.path.join(PATH_IN, 'cosine_similarity.csv'), 'a') as f:\n",
    "#            f.write(results)\n",
    "\n",
    "for i in range(len(rpeakList)):\n",
    "    subject= '_'.join(rpeakList[i].split('.')[0].split('_')[-2:])\n",
    "    rpeaks = pd.read_csv(rpeakList[i], header=None)\n",
    "    amplitudes = pd.read_csv(fxvalList[i], header=None)\n",
    "    \n",
    "    ## Task: 2. R-R amplitude Normalization\n",
    "    # min-max normalization \n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    # minmax normalization for fxvals(ECG Voltage)\n",
    "    amplitudes_MinMax = min_max_scaler.fit_transform(amplitudes)\n",
    "\n",
    "    \n",
    "    # Task: 1. R-R interval 나누기 + 3. R-R time frame(data points) Normalization             \n",
    "    # Task: 3. R-R time frame(data points) Normalization \n",
    "    # Return rr_intervals, yvals\n",
    "    rr_intervals = []\n",
    "    y_vals = []\n",
    "    for i in range(len(rpeaks)):\n",
    "        if i + 1 == len(rpeaks):\n",
    "            break\n",
    "        else:\n",
    "            rr_interval = rpeaks.iloc[i+1] - rpeaks.iloc[i]\n",
    "            y_val = amplitudes_MinMax[int(rpeaks.iloc[i]):int(rpeaks.iloc[i+1])]\n",
    "            rr_intervals.append(int(rr_interval))\n",
    "            y_vals.append(y_val)\n",
    "\n",
    "    if len(y_vals) == len(rr_intervals):\n",
    "        rr_interval_Norm = np.median(rr_intervals)\n",
    "    else: \n",
    "        print(\"{} Error: Not Same between the length of y_vals and the length of rr_intervals\".format(subject))\n",
    "\n",
    "    # Task: 3. R-R time frame(data points) Normalization \n",
    "        # # Test: check for resampling    \n",
    "        # y = y_vals[0]\n",
    "        # f = signal.resample(y_vals[0], int(np.median(rr_intervals)))\n",
    "\n",
    "        # len(f), len(y_vals[0])\n",
    "\n",
    "    # rr_interval_Norm: Reference\n",
    "    new_yvals = [] # new y-values based on normalized rr interval -> they have sample data ponits\n",
    "    org_len = []\n",
    "    for y_val in range(len(y_vals)):\n",
    "        org_len.append(len(y_vals[y_val]))\n",
    "        new_yval = signal.resample(y_vals[y_val], int(rr_interval_Norm))\n",
    "        new_yvals.append(new_yval)\n",
    "\n",
    "    # make matrix: [len(new_yvals) * len(reference; R-R interval Nromalziation)]\n",
    "    total_points = []\n",
    "    median_points = []\n",
    "    for len_rr_y_values in range(len(new_yvals[0])): # len(new_yvals) = 287\n",
    "        points = []\n",
    "        for len_y_values in range(len(new_yvals)): # # len(new_yvals) = 2771 \n",
    "            point = new_yvals[len_y_values][len_rr_y_values]\n",
    "            points.append(point)\n",
    "        total_points.append(points)\n",
    "        median_points.append(np.median(points))\n",
    "    \n",
    "\n",
    "    # calculate cosine simmlarities of each point comparing reference(new_yval) \n",
    "    cos_sims = []\n",
    "    ref = median_points\n",
    "    for new_yval in new_yvals:\n",
    "        cos_sim = cosine_similarity(np.array(ref).reshape(1,-1), np.array(new_yval).reshape(1,-1))\n",
    "        cos_sims.append(cos_sim)\n",
    "    \n",
    "    # when cos_sims exists over 1600, data makes two lines -> so, we divide cos_sims into each cos_sims dataset with 1000 cos_sims\n",
    "#     stems = [list(range(i*1000,(i + 1)*1000)) for i in list(range(divmod(len(cos_sims), 1000)[0]))]\n",
    "#     leafs = list(i+1 for i in range(divmod(len(cos_sims), 1000)[1]))\n",
    "\n",
    "\n",
    "#     results = subject + ',' +  str(int(rr_interval_Norm)) + ',' +  '; '.join(str(e) for e in org_len_yvals) +  ',' +  '; '.join(str(e) for e in np.concatenate(np.concatenate(cos_sims)).tolist()) + ',' + '\\n'\n",
    "#     with open (os.path.join(PATH_IN, 'cosine_similarity.csv'), 'a') as f:\n",
    "#                f.write(results)\n",
    "\n",
    "\n",
    "    # Make Sorted DataFrame and .csv file depending on each subject \n",
    "    df = pd.DataFrame({'index':range(len(cos_sims)), 'cos_sim':np.concatenate(np.concatenate(cos_sims)).tolist(), 'original_points':org_len})#,ascending=False)\n",
    "    df_sorted = df.sort_values(by=['cos_sim'], ascending=False)\n",
    "    tmp_index = tmp['index']\n",
    "    df_sorted.index= list(range(len(cos_sims)))\n",
    "\n",
    "    df_sorted.to_csv(os.path.join(PATH_IN, 'cosine_sim', 'cos_sim_{}_{}.csv'.format(subject, int(rr_interval_Norm))))\n",
    "    \n",
    "    print('{} is done. ; the length of Reference Frame: {}'.format(subject, int(rr_interval_Norm)))\n",
    "print(\"All finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f14f73f9-11ea-4e51-bd16-7c6cad66443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_cosine_similarity(PATH_IN):\n",
    "    \n",
    "    # load data\n",
    "    os.chdir(PATH_IN)\n",
    "    rpeakList = [i for i in os.listdir(PATH_IN) if i.startswith('correct')]\n",
    "    fxvalList = [i for i in os.listdir(PATH_IN) if i.startswith('filtered')]\n",
    "    \n",
    "    try: \n",
    "        os.mkdir(os.path.join(PATH_IN, 'cosine_sim'))\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    \n",
    "    for i in range(len(rpeakList)):\n",
    "        subject= '_'.join(rpeakList[i].split('.')[0].split('_')[-2:])\n",
    "        rpeaks = pd.read_csv(rpeakList[i], header=None)\n",
    "        amplitudes = pd.read_csv(fxvalList[i], header=None)\n",
    "\n",
    "        ## Task: 2. R-R amplitude Normalization\n",
    "        # min-max normalization \n",
    "        min_max_scaler = MinMaxScaler()\n",
    "        # minmax normalization for fxvals(ECG Voltage)\n",
    "        amplitudes_MinMax = min_max_scaler.fit_transform(amplitudes)\n",
    "\n",
    "\n",
    "        # Task: 1. R-R interval 나누기 + 3. R-R time frame(data points) Normalization             \n",
    "        # Task: 3. R-R time frame(data points) Normalization \n",
    "        # Return rr_intervals, yvals\n",
    "        rr_intervals = []\n",
    "        y_vals = []\n",
    "        for i in range(len(rpeaks)):\n",
    "            if i + 1 == len(rpeaks):\n",
    "                break\n",
    "            else:\n",
    "                rr_interval = rpeaks.iloc[i+1] - rpeaks.iloc[i]\n",
    "                y_val = amplitudes_MinMax[int(rpeaks.iloc[i]):int(rpeaks.iloc[i+1])]\n",
    "                rr_intervals.append(int(rr_interval))\n",
    "                y_vals.append(y_val)\n",
    "\n",
    "        if len(y_vals) == len(rr_intervals):\n",
    "            rr_interval_Norm = np.median(rr_intervals)\n",
    "        else: \n",
    "            print(\"{} Error: Not Same between the length of y_vals and the length of rr_intervals\".format(subject))\n",
    "\n",
    "        # rr_interval_Norm: Reference\n",
    "        new_yvals = [] # new y-values based on normalized rr interval -> they have sample data ponits\n",
    "        org_len = []\n",
    "        for y_val in range(len(y_vals)):\n",
    "            org_len.append(len(y_vals[y_val]))\n",
    "            new_yval = signal.resample(y_vals[y_val], int(rr_interval_Norm))\n",
    "            new_yvals.append(new_yval)\n",
    "\n",
    "        # make matrix: [len(new_yvals) * len(reference; R-R interval Nromalziation)]\n",
    "        total_points = []\n",
    "        median_points = []\n",
    "        for len_rr_y_values in range(len(new_yvals[0])): # len(new_yvals) = 287\n",
    "            points = []\n",
    "            for len_y_values in range(len(new_yvals)): # # len(new_yvals) = 2771 \n",
    "                point = new_yvals[len_y_values][len_rr_y_values]\n",
    "                points.append(point)\n",
    "            total_points.append(points)\n",
    "            median_points.append(np.median(points))\n",
    "\n",
    "\n",
    "        # calculate cosine simmlarities of each point comparing reference(new_yval) \n",
    "        cos_sims = []\n",
    "        ref = median_points\n",
    "        for new_yval in new_yvals:\n",
    "            cos_sim = cosine_similarity(np.array(ref).reshape(1,-1), np.array(new_yval).reshape(1,-1))\n",
    "            cos_sims.append(cos_sim)\n",
    "\n",
    "        # when cos_sims exists over 1600, data makes two lines -> so, we divide cos_sims into each cos_sims dataset with 1000 cos_sims\n",
    "    #     stems = [list(range(i*1000,(i + 1)*1000)) for i in list(range(divmod(len(cos_sims), 1000)[0]))]\n",
    "    #     leafs = list(i+1 for i in range(divmod(len(cos_sims), 1000)[1]))\n",
    "\n",
    "\n",
    "    #     results = subject + ',' +  str(int(rr_interval_Norm)) + ',' +  '; '.join(str(e) for e in org_len_yvals) +  ',' +  '; '.join(str(e) for e in np.concatenate(np.concatenate(cos_sims)).tolist()) + ',' + '\\n'\n",
    "    #     with open (os.path.join(PATH_IN, 'cosine_similarity.csv'), 'a') as f:\n",
    "    #                f.write(results)\n",
    "\n",
    "\n",
    "        # Make Sorted DataFrame and .csv file depending on each subject \n",
    "        df = pd.DataFrame({'index':range(len(cos_sims)), 'cos_sim':np.concatenate(np.concatenate(cos_sims)).tolist(), 'original_points':org_len})#,ascending=False)\n",
    "        df_sorted = df.sort_values(by=['cos_sim'], ascending=False)\n",
    "        tmp_index = tmp['index']\n",
    "        df_sorted.index= list(range(len(cos_sims)))\n",
    "\n",
    "        df_sorted.to_csv(os.path.join(PATH_IN, 'cosine_sim', 'cos_sim_{}_{}.csv'.format(subject, int(rr_interval_Norm))))\n",
    "\n",
    "        print('{} is done. ; the length of Reference Frame: {}'.format(subject, int(rr_interval_Norm)))\n",
    "    print(\"All finished\")    \n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "c65bfe57-c6d5-4017-8c71-0ce4808e53e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arrx_08 is done. ; the length of Reference Frame: 381\n",
      "arrx_09 is done. ; the length of Reference Frame: 253\n",
      "arrx_10 is done. ; the length of Reference Frame: 309\n",
      "arrx_11 is done. ; the length of Reference Frame: 253\n",
      "arrx_12 is done. ; the length of Reference Frame: 374\n",
      "arrx_13 is done. ; the length of Reference Frame: 389\n",
      "arrx_14 is done. ; the length of Reference Frame: 337\n",
      "arrx_15 is done. ; the length of Reference Frame: 271\n",
      "arrx_16 is done. ; the length of Reference Frame: 428\n",
      "arrx_19 is done. ; the length of Reference Frame: 356\n",
      "arrx_20 is done. ; the length of Reference Frame: 256\n",
      "arrx_21 is done. ; the length of Reference Frame: 430\n",
      "arrx_22 is done. ; the length of Reference Frame: 418\n",
      "arrx_38 is done. ; the length of Reference Frame: 310\n",
      "arrx_39 is done. ; the length of Reference Frame: 266\n",
      "arrx_40 is done. ; the length of Reference Frame: 294\n",
      "arrx_41 is done. ; the length of Reference Frame: 260\n",
      "arrx_42 is done. ; the length of Reference Frame: 307\n",
      "arrx_43 is done. ; the length of Reference Frame: 296\n",
      "arrx_44 is done. ; the length of Reference Frame: 354\n",
      "arrx_45 is done. ; the length of Reference Frame: 263\n",
      "arrx_46 is done. ; the length of Reference Frame: 212\n",
      "arrx_47 is done. ; the length of Reference Frame: 234\n",
      "All finished\n"
     ]
    }
   ],
   "source": [
    "cal_cosine_similarity(r'C:\\Users\\MI2RL-KHJ\\workspace_signal\\mit\\processed_arrx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41bc449-257a-47c1-960d-c91c07f73d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_rpeaks_test_arrx_ecg_arrx_08\n",
    "correct_rpeaks_test_arrx_ecg_arrx_08.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "36d46f1e-0a78-40b1-8d02-aeb2083bd524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm_00 is done. ; the length of Reference Frame: 89\n",
      "norm_01 is done. ; the length of Reference Frame: 127\n",
      "norm_02 is done. ; the length of Reference Frame: 96\n",
      "norm_03 is done. ; the length of Reference Frame: 92\n",
      "norm_04 is done. ; the length of Reference Frame: 82\n",
      "norm_05 is done. ; the length of Reference Frame: 123\n",
      "norm_06 is done. ; the length of Reference Frame: 101\n",
      "norm_07 is done. ; the length of Reference Frame: 107\n",
      "norm_08 is done. ; the length of Reference Frame: 89\n",
      "norm_09 is done. ; the length of Reference Frame: 121\n",
      "norm_10 is done. ; the length of Reference Frame: 86\n",
      "norm_11 is done. ; the length of Reference Frame: 89\n",
      "norm_12 is done. ; the length of Reference Frame: 96\n",
      "norm_13 is done. ; the length of Reference Frame: 81\n",
      "norm_14 is done. ; the length of Reference Frame: 94\n",
      "norm_15 is done. ; the length of Reference Frame: 111\n",
      "norm_16 is done. ; the length of Reference Frame: 92\n",
      "norm_17 is done. ; the length of Reference Frame: 69\n",
      "All finished\n"
     ]
    }
   ],
   "source": [
    "cal_cosine_similarity(r'C:\\Users\\MI2RL-KHJ\\workspace_signal\\mit\\processed_norm')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
